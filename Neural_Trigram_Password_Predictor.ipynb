{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW8P8lYg14r/+xFCH5CM/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KazDev17/Trigram-Neural-Network-Sequence-Predictor-/blob/main/Neural_Trigram_Password_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Character-Level Trigram Neural Network**"
      ],
      "metadata": {
        "id": "gEEpwpBUO7IU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**:\n",
        "To build a predictive model that understands the sequential probability of character patterns in common passwords.\n",
        "\n",
        "### **The Step Up**:\n",
        "While Bigram models only look at the previous character, this Trigram model uses a two-character context window, significantly increasing the model's structural understanding of strings."
      ],
      "metadata": {
        "id": "P5O_j_InPDeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 1: Loading the Dataset**"
      ],
      "metadata": {
        "id": "iS11Suv-supR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt -O data.txt\n",
        "\n",
        "words = open('data.txt', 'r').read().splitlines()\n",
        "print(f\"Loaded {len(words)} words.\")\n",
        "\n",
        "# 2. Build the Vocabulary\n",
        "# We find every unique character and map it to an integer\n",
        "chars = sorted(list(set(''.join(words) + '.')))\n",
        "stoi = {s:i for i,s in enumerate(chars)} # String to Integer\n",
        "itos = {i:s for i,s in enumerate(chars)} # Integer to String\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "NTRhJyu3O7ua",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 2: The \"Sliding Window\" Preprocessing**"
      ],
      "metadata": {
        "id": "5XKhpwSIs1lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most important part to grasp. In a standard Bigram neural network, we looked at 1 character to predict the next.\n",
        "\n",
        "In a Trigram, we look at 2 characters to predict the 3rd.\n",
        "\n",
        "To do this, we \"pad\" each word with special tokens (usually a .) so the model knows where a word starts."
      ],
      "metadata": {
        "id": "BUAP8J9fs_fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How the window moves through the word \"pass\":\n",
        "| Input (Context) | Output (Target) | Why? |\n",
        "| :--- | :--- | :--- |\n",
        "| .. | p | Start of the word |\n",
        "| .p | a | Context is now the start + 'p' |\n",
        "| pa | s | Context is the last two letters |\n",
        "| as | s | Moving forward... |\n",
        "| ss | . | Word is over |"
      ],
      "metadata": {
        "id": "TtD7QltjtTbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset\n",
        "block_size = 2 # Context length: How many characters we look at to predict the next?\n",
        "X, Y = [], []\n",
        "\n",
        "for w in words:\n",
        "    context = [0] * block_size # Start with padding '..'\n",
        "    for ch in w + '.':\n",
        "        ix = stoi[ch]\n",
        "        X.append(context)\n",
        "        Y.append(ix)\n",
        "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "        context = context[1:] + [ix] # Crop and append (sliding window)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "print(X.shape, Y.shape) # Should see [Number of Trigrams, 2]"
      ],
      "metadata": {
        "id": "9yuHst92VW7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 3: Embedding Layer**"
      ],
      "metadata": {
        "id": "btQtlwBjtnO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Rather than use \"One-Hot Encoding\" (a long string of 0s and a single 1), we create an Embedding Matrix ($C$).\n",
        "\n",
        " $C$ would be likened to a giant cabinet with 27 drawers (one for each character).\n",
        "\n",
        " Inside each drawer is a vector (a list of numbers) that represents that character's \"personality.\""
      ],
      "metadata": {
        "id": "IoO7XzCp2_yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We determine the dimensionality. 2 means each character is a (x, y) coordinate.\n",
        "emb_dim = 2\n",
        "\n",
        "# C is our Embedding Matrix.\n",
        "# It's a table of (vocab_size) rows and (emb_dim) columns.\n",
        "\n",
        "C = torch.randn((27, 2)) * 0.1 # set to 0.1 - 0.05\n",
        "\n",
        "# Now, we \"pluck\" the vectors out for our entire dataset X\n",
        "emb = C[X]\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Emb shape: {emb.shape}\")"
      ],
      "metadata": {
        "id": "5rhBLeEr26qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Flattening\n",
        "For every example, we have 2 characters, and each character has 2 numbers.\n",
        "\n",
        "To the Neural Network's next layer, this looks like a $2 \\times 2$ square.\n",
        "\n",
        "However, a standard \"Hidden Layer\" expects a single, flat line of numbers.\n",
        "\n",
        "We need to Concatenate them. If '**P**' is [0.5, -0.2] and '**A**' is [0.1, 0.9], we want to feed the model [0.5, -0.2, 0.1, 0.9]."
      ],
      "metadata": {
        "id": "CqZCBuZT9r37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use .view() to reshape the data as the emb tensor is currently 3D\n",
        "# -1 tells PyTorch \"figure out the number of rows automatically.\"\n",
        "# block_size * emb_dim (2 * 2 = 4) is our new input width.\n",
        "inputs = emb.view(-1, block_size * emb_dim)\n",
        "\n",
        "print(f\"Flattened input shape: {inputs.shape}\")"
      ],
      "metadata": {
        "id": "t6pkuSxD9q4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hidden Layer: Initializing the Brain**"
      ],
      "metadata": {
        "id": "zpUuyCsV8ojY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of neurons in our hidden layer (you can change this!)\n",
        "n_hidden = 300\n",
        "# Increase n_hidden between 200 to 500.\n",
        "\n",
        "\n",
        "# Multiply by 0.1 or 0.01 to \"quiet\" the initial random guesses\n",
        "W1 = torch.randn((block_size * emb_dim, n_hidden)) * 0.2\n",
        "b1 = torch.randn(n_hidden) * 0.01  # Small bias\n",
        "\n",
        "h = torch.tanh(inputs @ W1 + b1)\n",
        "\n",
        "print(f\"Hidden layer output shape: {h.shape}\")"
      ],
      "metadata": {
        "id": "hMOqYhlR8sGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 4: Output Layer**"
      ],
      "metadata": {
        "id": "F71UJcDI-cYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Weights: Input 1200, Output 27\n",
        "\n",
        "W2 = torch.randn((n_hidden, vocab_size)) * 0.01 # VERY small W2\n",
        "b2 = torch.randn(vocab_size) * 0    # Zero bias\n",
        "\n",
        "# Calculate the final scores\n",
        "logits = h @ W2 + b2\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\") # Output: [Total Examples, 27]"
      ],
      "metadata": {
        "id": "1HT2o7OM9GBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss Function**"
      ],
      "metadata": {
        "id": "77NCICFO_QA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = F.cross_entropy(logits, Y)\n",
        "print(f\"Initial Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "_wBoxRaT_Nie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "VtZcIv1f_azG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now, our weights ($W1, W2$) are just random numbers. The model is guessing blindly. We need to run a loop where the model:\n",
        "1. Forward Pass: Makes a guess.\n",
        "2. Backward Pass: Calculates which weights caused the mistake (Gradient).\n",
        "3. Update: Tweaks the weights slightly to be better next time (Optimization)."
      ],
      "metadata": {
        "id": "1N_eh8tT_gp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "\n",
        "    # --- 1. Create a list to store losses (before the loop starts) ---\n",
        "lossi = []\n",
        "\n",
        "recorded_losses = []\n",
        "\n",
        "# 2. The Training Loop\n",
        "for i in range(20000): # so it can learn better\n",
        "\n",
        "    # 1. Construct a Minibatch (Grab 32 random indexes)\n",
        "    ix = torch.randint(0, X.shape[0], (32,))\n",
        "\n",
        "    # 2. Forward Pass (Only on those 32 examples)\n",
        "    emb = C[X[ix]] # [32, 2, 2]\n",
        "    h = torch.tanh(emb.view(-1, 4) @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    loss = F.cross_entropy(logits, Y[ix])\n",
        "\n",
        "    # 3. Backward Pass\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Update (The Learning Rate)\n",
        "    # Start with -0.5 to -1.0\n",
        "    for p in parameters:\n",
        "        p.data += -0.7 * p.grad # toggle. the higher learning rater, the better.\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Step {i}: Loss {loss.item()}\")\n",
        "        recorded_losses.append(loss.item())\n",
        "\n",
        "    # We store the *log loss* because it makes the visual easier to read\n",
        "    # especially if the loss drops from very high (e.g., 17) to low (e.g., 2).\n",
        "    #lossi.append(loss.log10().item()) # --- Added this line! ---\n",
        "    lossi.append(loss.item()) # for a more dramatic downward curve\n",
        "\n",
        "\n",
        "print(f\"\\nAverage loss over steps 0 to 9000 (every 1000 steps): {sum(recorded_losses)/len(recorded_losses)}\")"
      ],
      "metadata": {
        "id": "mlOK_5M1mrWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ”® Trigram Password Predictor\n",
        "#@markdown Enter two characters to see how the model completes the sequence.\n",
        "\n",
        "user_input = \"pa\" #@param {type:\"string\"}\n",
        "generation_length = 4 # @param {\"type\":\"slider\",\"min\":4,\"max\":6,\"step\":5}\n",
        "\n",
        "# --- Logic to handle the input ---\n",
        "user_input = user_input.lower()\n",
        "if len(user_input) != 2:\n",
        "    print(\"Error: Please enter exactly TWO characters.\")\n",
        "else:\n",
        "    context = [stoi[c] for c in user_input]\n",
        "    word = user_input\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(generation_length):\n",
        "            emb = C[torch.tensor([context])]\n",
        "            h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
        "            logits = h @ W2 + b2\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            ix = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if ix == 0: break # Stop at '.'\n",
        "\n",
        "            word += itos[ix]\n",
        "            context = context[1:] + [ix]\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Input Context: '{user_input}'\")\n",
        "    print(f\"Generated Result: {word}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "AsRDsJkj_sH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Curve Graph"
      ],
      "metadata": {
        "id": "zOSitAHRjkh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The gray area represents the raw loss from each mini-batch, showing the inherent noise of Stochastic Gradient Descent (SGD).\n",
        "\n",
        "### The orange line is the smoothed moving average, showing a consistent convergence from an initial random loss of ~3.3 to an optimized final loss of ~2.28."
      ],
      "metadata": {
        "id": "P63CFGGBkHhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x9W_18pxHPSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# 1. Plot the raw log-losses\n",
        "# (It might look a bit 'noisy' if you are using Minibatches)\n",
        "plt.plot(lossi, label='Raw Log-Loss', color='gray', alpha=0.3)\n",
        "\n",
        "# 2. Add a 'smoothed' moving average\n",
        "# This helps you see the actual trend (smooth downward slide)\n",
        "# We average every 100 steps.\n",
        "smoothing_window = 100\n",
        "smoothed_loss = np.convolve(lossi, np.ones(smoothing_window)/smoothing_window, mode='valid')\n",
        "plt.plot(np.arange(smoothing_window-1, len(lossi)), smoothed_loss, label=f'Smoothed (Avg {smoothing_window})', color='orange', linewidth=2)\n",
        "\n",
        "plt.title(f\"Trigram MLP Training Loss (Final Smooth Loss: {np.mean(lossi[-100:]):.4f})\", fontsize=14)\n",
        "plt.xlabel(\"Training Steps (Iterations)\", fontsize=12)\n",
        "plt.ylabel(\"Log Cross-Entropy Loss\", fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM1XH0B-HNNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-m8hbtazkoeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference: Name Generation"
      ],
      "metadata": {
        "id": "9pvvT6SJkjbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Name Generation (Inference) ---\n",
        "\n",
        "for _ in range(10): # Generate 10 names\n",
        "    out = []\n",
        "    context = [0] * block_size # Start with \"..\" (encoded as [0, 0])\n",
        "\n",
        "    while True:\n",
        "        # 1. Forward pass: Get the \"thoughts\" of the model for current context\n",
        "        emb = C[torch.tensor([context])] # [1, block_size, n_emb]\n",
        "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
        "        logits = h @ W2 + b2\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # 2. Sample from the distribution (don't just take the highest!)\n",
        "        ix = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        # 3. Shift the context window\n",
        "        context = context[1:] + [ix]\n",
        "\n",
        "        # 4. Break if we hit the end-of-word token '.'\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "        out.append(itos[ix])\n",
        "\n",
        "    print(''.join(out))"
      ],
      "metadata": {
        "id": "WghdYmZaf4pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scatter Plot**\n",
        "\n",
        "Proving that the neural network didn't just memorize strings, but actually developed a geometric understanding of the alphabet."
      ],
      "metadata": {
        "id": "gktuH-zqv4qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 1. Create the figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# 2. Extract the x and y coordinates from our Embedding Matrix C\n",
        "# We use .data to get the raw numbers and avoid gradient tracking\n",
        "x = C[:,0].data\n",
        "y = C[:,1].data\n",
        "\n",
        "# 3. Create the scatter plot\n",
        "plt.scatter(x, y, s=400, c='skyblue', alpha=0.6)\n",
        "\n",
        "# 4. Label each point with its corresponding character\n",
        "for i in range(vocab_size):\n",
        "    plt.text(x[i].item(), y[i].item(), itos[i], ha=\"center\", va=\"center\", color='black', fontsize=12, weight='bold')\n",
        "\n",
        "plt.title(\"Character Embedding Space (2D Representation)\", fontsize=15)\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hyYOZqGov7Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This project demonstrates the implementation of a Trigram Neural Network from scratch.\n",
        "\n",
        "### By increasing the context window to two characters and utilizing a 2-dimensional embedding space, the model achieved a 30% reduction in cross-entropy loss compared to a baseline Bigram approach.\n",
        "\n",
        "### The resulting embedding visualization confirms that the model successfully learned phonetic structures, such as vowel-consonant relationships, without explicit programming."
      ],
      "metadata": {
        "id": "u1L7tgBOys81"
      }
    }
  ]
}